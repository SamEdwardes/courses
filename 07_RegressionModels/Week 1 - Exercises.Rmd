---
title: "Week 1 - Exercises"
output: html_document
---

# Notation
## Q #1
Take the Galton dataset and find the mean, standard deviation and correlation between the parental and child heights.
```{r, message=FALSE}
library(UsingR)
data("galton")
head(galton)

# data
child <- galton$child
parent <- galton$parent
n <- length(child)

# mean
x_bar <- mean(child); x_bar
y_bar <- mean(parent); y_bar

# sd
x_sigma <- sd(child); x_sigma
y_sigma <- sd(parent); y_sigma

# correlation
covariance <- (sum((child - x_bar)*(parent - y_bar)))/(n-1); covariance
covariance_alt <- (sum(child*parent) - n*x_bar*y_bar)/(n-1); covariance_alt
correlation <- covariance/(x_sigma*y_sigma); correlation

# using R function
cor(galton)
```

## Q #2
Center the parent and child variables and verify that the centered variable means are 0.
```{r, message=FALSE}
library(UsingR)
data("galton")

# data
child <- galton$child
parent <- galton$parent
n <- length(child)

# mean
x_bar <- mean(child); x_bar
y_bar <- mean(parent); y_bar

# centre the data
child <- child - x_bar
parent <- parent - y_bar

# mean is now at zero
round(mean(child),2)
round(mean(parent),2)
```

## Q #3
Rescale the parent and child variables and verify that the scaled variable standard deviations are 1
```{r, message=FALSE}
library(UsingR)
data("galton")

# data
child <- galton$child
parent <- galton$parent
n <- length(child)

# sd
x_sigma <- sd(child); x_sigma
y_sigma <- sd(parent); y_sigma

# scale the data
child <- child/x_sigma
parent <- parent/y_sigma

# new SDs are 1
sd(child)
sd(parent)
```

## Q #4

```{r, message=FALSE}
library(UsingR)
data("galton")

# data
child <- galton$child
parent <- galton$parent
n <- length(child)

# mean
x_bar <- mean(child); x_bar
y_bar <- mean(parent); y_bar

# sd
x_sigma <- sd(child); x_sigma
y_sigma <- sd(parent); y_sigma

# normalize the data
parent <- (parent - y_bar)/y_sigma
child <- (child - x_bar)/x_sigma

# mean is 0 and sd is 1
rbind(cbind(mean(parent), sd(parent)),
      cbind(mean(child), sd(child)))

# correlation
cor(data.frame(parent,child))
cor(parent, child)
```

# Ordinary Least Squares
## Q #1
InstallandloadthepackageUsingRandloadthefather.sondatawithdata(father.son).Get the linear regression fit where the son’s height is the outcome and the father’s height is the predictor. Give the intercept and the slope, plot the data and overlay the fitted regression line.
```{r, messages=FALSE}
library(UsingR)
library(ggplot2)
data("father.son")
# head(father.son)
x <- father.son$fheight
y <- father.son$sheight
df <- data.frame(x,y)

# get linear regression fit
fs_lm <- lm(y ~ x); fs_lm

# plot the data and lm line
g <- ggplot(data=df, mapping=aes(x, y)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method='lm', formula = y~x, se=FALSE) +
  xlim(0,max(x)) + 
  ylim(0,max(y))
g
```

## Q #2
```{r, message=FALSE}
library(UsingR)
library(ggplot2)
data("father.son")
x <- father.son$fheight
y <- father.son$sheight

# centre the data
x <- x - mean(x)
y <- y - mean(y)
df <- data.frame(x,y)

# get linear regression fit
fsC_lm <- lm(y ~ x)
fs_lm <- lm(sheight ~ fheight, data = father.son)
round(fsC_lm$coefficients,4)
round(fs_lm$coefficients,4)

# plot the data and lm line
g <- ggplot(data=df, mapping=aes(x, y)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method='lm', formula = y~x, se=FALSE) +
  xlim(0,max(x)) + 
  ylim(0,max(y))
g
```

## Q #3
```{r, message=FALSE}
library(UsingR)
library(ggplot2)
data("father.son")
x <- father.son$fheight
y <- father.son$sheight

# normalize the data
x <- (x - mean(x))/sd(x)
y <- (y - mean(y))/sd(y)
df <- data.frame(x,y)

# get linear regression fit
fsN_lm <- lm(y ~ x)
fs_lm <- lm(sheight ~ fheight, data = father.son)
round(fsN_lm$coefficients,4)
round(fs_lm$coefficients,4)
cor(y, x)
cor(father.son$sheight, father.son$fheight)

# plot the data and lm line
g <- ggplot(data=df, mapping=aes(x, y)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method='lm', formula = y~x, se=FALSE) +
  xlim(0,max(x)) + 
  ylim(0,max(y))
g
```

## Q #4
Go back to the linear regression line from Problem 1. If a father’s height was 63 inches, what would you predict the son’s height to be?
```{r, messages=FALSE}
library(UsingR)
library(ggplot2)
data("father.son")
# head(father.son)
x <- father.son$fheight
y <- father.son$sheight
df <- data.frame(x,y)

# get linear regression fit
fs_lm <- lm(y ~ x); fs_lm

# plot the data and lm line
g <- ggplot(data=df, mapping=aes(x, y)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method='lm', formula = y~x, se=FALSE)
g

# if fathers height is 63... than we predict sons height to be...
x1 <- 63
y1 <- fs_lm$coefficients[[1]] + x1 * fs_lm$coefficients[[2]]; y1

# using predict function
predict(fs_lm, newdata = data.frame(x = 63))
```

## Q #5
Consider a data set where the standard deviation of the outcome variable is double that of the predictor. Also, the variables have a correlation of 0.3. If you fit a linear regression model, what would be the estimate of the slope? 
```{r}
x_sd <- 1
y_sd <- x_sd*2
cor_yx <- 0.3

# in a lm what would the slope be?
# b1 = cor(y,x)*(sd(y)/sd(x))
b1 <- 0.3 * y_sd/x_sd
b1
```

## Q #6
Consider the previous problem. The outcome variable has a mean of 1 and the predictor has a mean of 0.5. What would be the intercept?
```{r}
x_sd <- 1
y_sd <- x_sd*2
y_bar <- 1
x_bar <- 0.5
cor_yx <- 0.3
# b1 = cor(y,x)*(sd(y)/sd(x))
b1 <- 0.3 * y_sd/x_sd #0.6

# what is the intercept?
# b0 = y_bar - b1*x_bar
b0 <- y_bar - b1*x_bar
b0
```

## Q #7
Trueorfalse,ifthepredictorvariablehasmean0,theestimatedinterceptfromlinearregression will be the mean of the outcome? 

> b0 = y_bar - b1*x_bar

> TRUE

## Q #8
Consider problem 5 again. What would be the estimated slope if the predictor and outcome were reversed?
```{r}
x_sd <- 1
y_sd <- x_sd*2
cor_yx <- 0.3

# in a lm what would the slope be?
# b1 = cor(y,x)*(sd_x/sd_y)
b1 <- 0.3 * (x_sd/y_sd)
b1
```

# Regresion to the mean
## Q #1
You have two noisy scales and a bunch of people that you’d like to weigh. You weigh each person on both scales. The correlation was 0.75. If you normalized each set of weights, what would you have to multiply the weight on one scale to get a good estimate of the weight on the other scale?

> You would have to multiply the weight on one scale by 0.75 (the correlation, also the slope).

## Q#2
Consider the previous problem. Someone’s weight was 2 standard deviations above the mean of the group on the first scale. How many standard deviations above the mean would you estimate them to be on the second?

> 2 *.75

## Q #3
You ask a collection of husbands and wives to guess how many jellybeans are in a jar. The correlation is 0.2. The standard deviation for the husbands is 10 beans while the standard deviation for wives is 8 beans. Assume that the data were centered so that 0 is the mean for each. The centered guess for a husband was 30 beans (above the mean). What would be your best estimate of the wife’s guess?

```{r}
rho <- 0.2 # correlation
x_sigma <- 10 # x is husbands
y_sigma <- 8 # y is wives
mu <- 0

# a husbands centered guess was 30 beans above the mean. What would be your estimte of the wife's guess?
b1 <- rho * (y_sigma/x_sigma)
b1

xi <- 30
yi <- xi * b1 # no need to add intercept b/c its zero
yi

```

# Quiz 1
## Q #1
```{r}
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)

mu <- sum(x*w)/sum(w)
mu
```

## Q #2
```{r}
# without centering
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)

lm(y ~ x - 1) # use -1 to go throught the origin

# with centering
x <- x - mean(x)
y <- y - mean(y)
lm(y ~ x)
```

## Q #3
```{r}
data(mtcars)
x <- mtcars$wt
y <- mtcars$mpg

lm(y ~ x)
```

## Q #4
```{r}
x_sigma <- 0.5* y_sigma
rho <- 0.5 # correlation

# answer is 1, see notes
```

## Q #5
Students were given two hard tests and scores were normalized to have empirical mean 0 and variance 1. The correlation between the scores on the two tests was 0.4. What would be the expected score on Quiz 2 for a student who had a normalized score of 1.5 on Quiz 1?
```{r}
1.5 * 0.4
```

## Q #6
```{r}
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)

x_z <- (x - mean(x))/sd(x)
x_z
```

## Q #7
```{r}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)

lm(y ~ x)
```

## Q #8
You know that both the predictor and response have mean 0. What can be said about the intercept when you fit a linear regression?

> it will pass through 0,0

## Q #9
```{r}
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)

mean(x)
```

## Q #10
```{r}
# create some mock data
rho <- 0.2 # correlation
sd_x <- 1
sd_y <- 2

# b1 = corr(y,x) * (sd(y)/sd(x))
# y1 = corr(y,x) * (sd(x)/sd(y))

b1 <- 0.2 * sd_y/sd_x
y1 <- 0.2 * sd_x/sd_y
b1
y1

# b1/y1 (the ratio) is equal to var(Y)/var(X)
# test the theory

c(b1/y1, sd_y^2 / sd_x^2)

# test again with new varaibles
sd_x <- 3
sd_y <- 2
b1 <- 0.2 * sd_y/sd_x
y1 <- 0.2 * sd_x/sd_y
c(b1/y1, sd_y^2 / sd_x^2)
```

